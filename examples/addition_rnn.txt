training data :
(50k, 7, 12)

[Q] why use one-hot vector ?
[Q] batch_size is related to lstm_size(output dim of lstm cell) ??

[Q] for an input digits of length 7, say  '361+279',
it is converted to 7 one-hot vector of length 12,
7 == time_step_size
each one-hot vector is fed into LSTM/RNN(unrolled) step by step
after 7 steps of recurrence, 

input the next 7-digit , etc.

...

so the batch_size is just to separate the inputs into batches,
seems nothing related to lstm_size

//===
print(X_val[1,:,:])

[[False False False False False False False False False  True False False]
 [False False False False False False False False False False  True False]
 [False False False False False False  True False False False False False]
 [False  True False False False False False False False False False False]
 [False False False False False False False False False  True False False]
 [False False False False False False False False False  True False False]
 [False False False False False False False False False False  True False]]
 

myrowX=X_val[[1]]
print(myrowX[0])

[[False False False False False False False False False  True False False]
 [False False False False False False False False False False  True False]
 [False False False False False False  True False False False False False]
 [False  True False False False False False False False False False False]
 [False False False False False False False False False  True False False]
 [False False False False False False False False False  True False False]
 [False False False False False False False False False False  True False]]
 
 [Q] seems no difference between ?
 myrowX=X_val[[1]]
 myrowX=X_val[np.array([1])]
 
//===
indices = np.arange(len(y))
np.random.shuffle(indices)

>>> np.arrange(4)
AttributeError: 'module' object has no attribute 'arrange'

>>> np.arange(4)
array([0, 1, 2, 3])
>>> np.arange(5)
array([0, 1, 2, 3, 4])
>>> np.arange(3)
array([0, 1, 2])

//===
# "Encode" the input sequence using an RNN, producing an output of HIDDEN_SIZE
# note: in a situation where your input sequences have a variable length,
# use input_shape=(None, nb_feature).
# here , nb_feature=12 = len(' +0123456789')
# HIDDEN_SIZE=128, MAXLEN=7 = DIGITS*2 +1
model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))

# For the decoder's input, we repeat the encoded input for each time step
model.add(RepeatVector(DIGITS + 1))


//=== layers/core.py
class RepeatVector(Layer):
    '''Repeats the input n times.
